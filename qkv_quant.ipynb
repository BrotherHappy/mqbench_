{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import mqbench,torch,torchvision,numpy as np,matplotlib.pyplot as plt,torchvision,torchvision.models as models,timm,timm.models as models\n",
    "%matplotlib inline\n",
    "from mqbench.prepare_by_platform import prepare_by_platform\n",
    "from mqbench.prepare_by_platform import BackendType\n",
    "from mqbench.utils.state import enable_calibration\n",
    "from mqbench.utils.state import enable_quantization\n",
    "from mqbench.convert_deploy import convert_deploy\n",
    "from tqdm import tqdm\n",
    "from mqbench.fake_quantize.lsq import LearnableFakeQuantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch,torch.nn as nn\n",
    "class MyModel(nn.Module):\n",
    "    def forward(self,x,y):\n",
    "        return torch.matmul(x,y)\n",
    "        # return x + y\n",
    "model = MyModel()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MQBENCH] INFO: Quantize model Scheme: BackendType.Vitis Mode: Training\n",
      "[MQBENCH] INFO: Weight Quant Scheme is overrided!\n",
      "[MQBENCH] INFO: Activation Quant Scheme is overrided!\n",
      "[MQBENCH] INFO: Weight Qconfig:\n",
      "    FakeQuantize: FixedFakeQuantize Params: {}\n",
      "    Oberver:      MinMaxObserver Params: Symmetric: True / Bitwidth: 8 / Per channel: True / Pot scale: False / Extra kwargs: {}\n",
      "[MQBENCH] INFO: Activation Qconfig:\n",
      "    FakeQuantize: LearnableFakeQuantize Params: {}\n",
      "    Oberver:      EMAMinMaxObserver Params: Symmetric: True / Bitwidth: 8 / Per channel: False / Pot scale: False / Extra kwargs: {}\n",
      "[MQBENCH] INFO: Bias Qconfig:\n",
      "    TqtFakeQuantize with MinMaxObserver\n",
      "[MQBENCH] INFO: Replace module to qat module.\n",
      "[MQBENCH] INFO: Add matmul_1 to output quantize\n",
      "[MQBENCH] INFO: Insert act quant matmul_1_post_act_fake_quantizer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GraphModule(\n",
       "  (matmul_1_post_act_fake_quantizer): LearnableFakeQuantize(\n",
       "    fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=Parameter containing:\n",
       "    tensor([1.], requires_grad=True), zero_point=Parameter containing:\n",
       "    tensor([0.], requires_grad=True)\n",
       "    (activation_post_process): EMAMinMaxObserver(min_val=inf, max_val=-inf ch_axis=-1 pot=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "extra_qconfig_dict = {\n",
    "    'w_observer': 'MinMaxObserver',\n",
    "    'a_observer': 'EMAMinMaxObserver',\n",
    "    'w_fakequantize': 'FixedFakeQuantize',\n",
    "    'a_fakequantize': 'LearnableFakeQuantize',\n",
    "    'w_qscheme': {\n",
    "        'bit': 8,\n",
    "        'symmetry': True,\n",
    "        'per_channel': True,\n",
    "        'pot_scale': False\n",
    "    },\n",
    "    'a_qscheme': {\n",
    "        'bit': 8,\n",
    "        'symmetry': True,\n",
    "        'per_channel': False,\n",
    "        'pot_scale': False\n",
    "    }\n",
    "}\n",
    "device = torch.device('cpu')\n",
    "prepare_custom_config_dict = {'extra_qconfig_dict': extra_qconfig_dict}\n",
    "model = prepare_by_platform(model, BackendType.Vitis, prepare_custom_config_dict).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphModule(\n",
       "  (matmul_1_post_act_fake_quantizer): LearnableFakeQuantize(\n",
       "    fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=Parameter containing:\n",
       "    tensor([1.], requires_grad=True), zero_point=Parameter containing:\n",
       "    tensor([0.], requires_grad=True)\n",
       "    (activation_post_process): EMAMinMaxObserver(min_val=inf, max_val=-inf ch_axis=-1 pot=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch,torch.nn as nn,torchvision as tv\n",
    "model = tv.models.resnet18()\n",
    "\n",
    "def _swap_ff_with_fxff(model: torch.nn.Module) -> None:\n",
    "    r\"\"\" Swap FloatFunctional with FXFloatFunctional\n",
    "    \"\"\"\n",
    "    modules_to_swap = []\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, torch.nn.quantized.FloatFunctional):\n",
    "            modules_to_swap.append(name)\n",
    "            print(name)\n",
    "        else:\n",
    "            _swap_ff_with_fxff(module)\n",
    "\n",
    "    for name in modules_to_swap:\n",
    "        del model._modules[name]\n",
    "        model._modules[name] = torch.nn.quantized.FXFloatFunctional()\n",
    "\n",
    "_swap_ff_with_fxff(model) # "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
